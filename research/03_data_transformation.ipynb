{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/omidsardari/WORK/Becoming a Data Scientist/Python Projects/End_to_End_Transformer_En_Fa'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    tokenizer_file: Path\n",
    "    dataset_name: str\n",
    "    lang_src: str\n",
    "    lang_tgt: str\n",
    "    seq_len: int\n",
    "    batch_size: int\n",
    "    train_val_split_ratio: Tuple[float, float]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformerEnFa.constants import *\n",
    "from transformerEnFa.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        return DataTransformationConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            tokenizer_file = config.tokenizer_file,\n",
    "            dataset_name = config.dataset_name,\n",
    "            lang_src = config.lang_src,\n",
    "            lang_tgt = config.lang_tgt,\n",
    "            seq_len = config.seq_len,\n",
    "            batch_size = config.batch_size,\n",
    "            train_val_split_ratio = tuple(config.train_val_split_ratio),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface Tokenizers\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    print(\"Inspecting the first item of ds:\", ds[0])  # Add this line for debugging\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config.tokenizer_file.format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        # Huggigface code\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        # added line of code \n",
    "        tokenizer_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BilingualDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.sos_token = torch.tensor([tokenizer_src.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_src.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_src.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "     \n",
    "        # text to tokens\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "        \n",
    "        # Calculate padding number\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "        \n",
    "        #  Raise Error if the number get negative\n",
    "\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError('Sentence is too long base on the sequence length has defined ') \n",
    "        \n",
    "        # Add SOS, EOS and PADDINGS to the source text\n",
    "        encoder_input = torch.concat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64)\n",
    "\n",
    "                  ]\n",
    "        )\n",
    "\n",
    "        decoder_input = torch.concat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
    "\n",
    "                  ]\n",
    "        )\n",
    "\n",
    "        label = torch.concat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
    "\n",
    "                  ]\n",
    "        )\n",
    "\n",
    "        # Check the size of the tensors to make sure they are all seq_len \n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }\n",
    "\n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_max_lengths(self, ds_raw):\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "    for item in ds_raw:\n",
    "        src_ids = self.tokenizer_src.encode(item['translation'][self.config.lang_src]).ids\n",
    "        tgt_ids = self.tokenizer_tgt.encode(item['translation'][self.config.lang_tgt]).ids\n",
    "        max_len_src = max(len(src_ids), max_len_src)\n",
    "        max_len_tgt = max(len(tgt_ids), max_len_tgt)\n",
    "    return max_len_src, max_len_tgt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,  random_split\n",
    "import pandas as pd\n",
    "from transformerEnFa.logging import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def get_ds(config):\\n    # Dataset only has a train split, so we divide it\\n    ds_raw = load_dataset(config.dataset_name, split=\\'train[:5%]\\')\\n\\n        # It only has the train split, so we divide it overselves\\n    \\n    # Build Tokenoizers\\n    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config.lang_src)\\n    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config.lang_tgt)\\n\\n    # 90% training, 10% validation\\n    train_ds_size = int(0.9 * len(ds_raw))\\n    val_ds_size = len(ds_raw) - train_ds_size\\n    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\\n\\n\\n    # Convert train and validate dataset to tokens by padding, mask, SOS, EOS and etc.\\n    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config.lang_src, config.lang_tgt, config.seq_len)\\n    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config.lang_src, config.lang_tgt, config.seq_len)\\n\\n    # Find the maximum length of each sentence in the source and target sentence\\n    max_len_src = 0\\n    max_len_tgt = 0\\n\\n    for item in ds_raw:\\n       src_ids = tokenizer_src.encode(item[\\'translation\\'][config.lang_src]).ids\\n       tgt_ids = tokenizer_tgt.encode(item[\\'translation\\'][config.lang_tgt]).ids\\n       max_len_src = max(len(src_ids), max_len_src)\\n       max_len_tgt = max(len(tgt_ids), max_len_tgt)\\n    logger.info(f\"Max length of source sentence: {max_len_src}\")\\n    logger.info(f\"Max length of target sentence: {max_len_tgt}\")\\n\\n\\n    train_dataloader = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True)\\n    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\\n\\n    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def get_ds(config):\n",
    "    # Dataset only has a train split, so we divide it\n",
    "    ds_raw = load_dataset(config.dataset_name, split='train[:5%]')\n",
    "\n",
    "        # It only has the train split, so we divide it overselves\n",
    "    \n",
    "    # Build Tokenoizers\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config.lang_src)\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config.lang_tgt)\n",
    "\n",
    "    # 90% training, 10% validation\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "\n",
    "    # Convert train and validate dataset to tokens by padding, mask, SOS, EOS and etc.\n",
    "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config.lang_src, config.lang_tgt, config.seq_len)\n",
    "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config.lang_src, config.lang_tgt, config.seq_len)\n",
    "\n",
    "    # Find the maximum length of each sentence in the source and target sentence\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "\n",
    "    for item in ds_raw:\n",
    "       src_ids = tokenizer_src.encode(item['translation'][config.lang_src]).ids\n",
    "       tgt_ids = tokenizer_tgt.encode(item['translation'][config.lang_tgt]).ids\n",
    "       max_len_src = max(len(src_ids), max_len_src)\n",
    "       max_len_tgt = max(len(tgt_ids), max_len_tgt)\n",
    "    logger.info(f\"Max length of source sentence: {max_len_src}\")\n",
    "    logger.info(f\"Max length of target sentence: {max_len_tgt}\")\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = ConfigurationManager()\n",
    "#data_transformation_config = config.get_data_transformation_config()\n",
    "#get_ds(config=data_transformation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,  random_split\n",
    "from transformerEnFa.logging import logger\n",
    "from datasets import load_dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataTransformationTrainingPipeline():\n",
    "    def __init__(self):\n",
    "        self.config_manager = ConfigurationManager()\n",
    "        self.config = self.config_manager.get_data_transformation_config()\n",
    "\n",
    "    def calculate_max_lengths(self, ds_raw):\n",
    "        max_len_src = 0\n",
    "        max_len_tgt = 0\n",
    "        for item in ds_raw:\n",
    "            src_ids = self.tokenizer_src.encode(item['translation'][self.config.lang_src]).ids\n",
    "            tgt_ids = self.tokenizer_tgt.encode(item['translation'][self.config.lang_tgt]).ids\n",
    "            max_len_src = max(len(src_ids), max_len_src)\n",
    "            max_len_tgt = max(len(tgt_ids), max_len_tgt)\n",
    "        return max_len_src, max_len_tgt\n",
    "\n",
    "    def get_ds(self):\n",
    "        # Load the dataset\n",
    "        ds_raw = load_dataset(self.config.dataset_name, split='train[:5%]')\n",
    "\n",
    "        # Build tokenizers for source and target languages\n",
    "        self.tokenizer_src = get_or_build_tokenizer(self.config, ds_raw, self.config.lang_src)\n",
    "        self.tokenizer_tgt = get_or_build_tokenizer(self.config, ds_raw, self.config.lang_tgt)\n",
    "\n",
    "        # Split the dataset into training and validation sets\n",
    "        train_ds_size = int(0.9 * len(ds_raw))\n",
    "        val_ds_size = len(ds_raw) - train_ds_size\n",
    "        train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "        # Prepare the datasets for training and validation\n",
    "        train_ds = BilingualDataset(train_ds_raw, self.tokenizer_src, self.tokenizer_tgt, self.config.lang_src, self.config.lang_tgt, self.config.seq_len)\n",
    "        val_ds = BilingualDataset(val_ds_raw, self.tokenizer_src, self.tokenizer_tgt, self.config.lang_src, self.config.lang_tgt, self.config.seq_len)\n",
    "\n",
    "        # Calculate the maximum sentence lengths\n",
    "        max_len_src, max_len_tgt = self.calculate_max_lengths(ds_raw)\n",
    "\n",
    "        # Log the maximum sentence lengths\n",
    "        print(f\"Max length of source sentence: {max_len_src}\")\n",
    "        print(f\"Max length of target sentence: {max_len_tgt}\")\n",
    "\n",
    "        # Create DataLoaders for the training and validation datasets\n",
    "        self.train_dataloader = DataLoader(train_ds, batch_size=self.config.batch_size, shuffle=True)\n",
    "        self.val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "        return self.train_dataloader, self.val_dataloader\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-09 14:13:21,714: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-02-09 14:13:21,716: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-02-09 14:13:21,717: INFO: common: created directory at: artifacts]\n",
      "[2024-02-09 14:13:21,717: INFO: common: created directory at: artifacts/data_transformation]\n",
      "Max length of source sentence: 37\n",
      "Max length of target sentence: 33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x11c61a450>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x169369150>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transformation = DataTransformationTrainingPipeline()\n",
    "data_transformation.get_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
